{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08857ca1",
   "metadata": {},
   "source": [
    "# **1. ë°ì´í„° ì²˜ë¦¬**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79c6ca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# ==========================================\n",
    "# [1] ì„¤ì • (Configuration)\n",
    "# ==========================================\n",
    "# ë¡œê·¸ íŒŒì¼ë“¤ì´ ìˆëŠ” í´ë” ê²½ë¡œ (ë§ˆì§€ë§‰ì— / ì—†ì´ ì…ë ¥)\n",
    "LOG_FOLDER_PATH = \"C:/Users/qkrwl/Desktop/telemetry_for_top15\"  # ì˜ˆ: 'C:/Users/User/Desktop/logs'\n",
    "\n",
    "OUTPUT_CSV = 'data_v3.csv'\n",
    "\n",
    "# íŒŒë¼ë¯¸í„°\n",
    "TIME_INTERVAL_SEC = 10     \n",
    "START_ALIVE_THRESHOLD = 10 \n",
    "AI_TEAM_ID_START = 200\n",
    "MAX_AI_RATIO = 0.4         # [ì¶”ê°€] ë´‡ ë¹„ìœ¨ ì œí•œ (0.4 ì´ˆê³¼ ì‹œ ìŠ¤í‚µ)\n",
    "STALE_THRESHOLD_SEC = 30   # [v1 ì°¸ê³ ] 60ì´ˆ ì´ìƒ ì—…ë°ì´íŠ¸ ì—†ìœ¼ë©´ ì£½ì€ ê²ƒìœ¼ë¡œ ê°„ì£¼\n",
    "\n",
    "# ì¶”ì í•  ì†Œëª¨í’ˆ\n",
    "TRACK_CONSUMABLES = {\n",
    "    # [í]\n",
    "    'Item_Heal_MedKit_C': 'medkit',\n",
    "    'Item_Heal_FirstAid_C': 'firstaid',\n",
    "    'Item_Boost_AdrenalineSyringe_C': 'adrenaline',\n",
    "    'Item_Boost_PainKiller_C': 'painkiller',\n",
    "    'Item_Boost_EnergyDrink_C': 'drink',\n",
    "\n",
    "    # [íˆ¬ì²™]\n",
    "    'Item_Weapon_Grenade_C': 'grenade',\n",
    "    'Item_Weapon_SmokeBomb_C': 'smoke',\n",
    "    'Item_Weapon_Molotov_C': 'molotov',\n",
    "    'Item_Weapon_FlashBang_C': 'flash'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc443926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# [2] í—¬í¼ í•¨ìˆ˜\n",
    "# ==========================================\n",
    "def get_item_key(item_id):\n",
    "    return TRACK_CONSUMABLES.get(item_id, None)\n",
    "\n",
    "def get_armor_info(item_id):\n",
    "    if not isinstance(item_id, str): return None, 0\n",
    "    level = 0\n",
    "    if 'Lv3' in item_id: level = 3\n",
    "    elif 'Lv2' in item_id: level = 2\n",
    "    elif 'Lv1' in item_id: level = 1\n",
    "    \n",
    "    if 'Head' in item_id: return 'helm', level\n",
    "    if 'Armor' in item_id or 'Vest' in item_id: return 'vest', level\n",
    "    return None, 0\n",
    "\n",
    "def update_player_state(p_state, name, char_obj):\n",
    "    \"\"\"í”Œë ˆì´ì–´ ìƒíƒœ(ì²´ë ¥, ìœ„ì¹˜, ê¸°ì ˆ) ì—…ë°ì´íŠ¸ ê³µí†µ í•¨ìˆ˜\"\"\"\n",
    "    if name in p_state and char_obj:\n",
    "        if 'health' in char_obj:\n",
    "            p_state[name]['hp'] = char_obj['health']\n",
    "        if 'location' in char_obj:\n",
    "            p_state[name]['loc'] = char_obj['location']\n",
    "        if 'isDBNO' in char_obj:\n",
    "            p_state[name]['groggy'] = 1 if char_obj['isDBNO'] else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8af09898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# [3] ë©”ì¸ ë¶„ì„ ë¡œì§ (v1 ê¸°ë°˜, ì•„ì´í…œë§Œ ì¶”ê°€)\n",
    "# ==========================================\n",
    "import orjson\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def process_match_log(file_path):\n",
    "    # --- Step 1: íŒŒì¼ ì½ê¸° (orjson ì‚¬ìš©) ---\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            logs = [orjson.loads(line) for line in f if line.strip()]\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "    # --- Step 2: AI ë¹„ìœ¨ ì²´í¬ ---\n",
    "    all_players = set()\n",
    "    human_players = set()\n",
    "    for log in logs:\n",
    "        if log.get('_T') == 'LogPlayerPosition':\n",
    "            c = log.get('character') or {}\n",
    "            name = c.get('name')\n",
    "            if name:\n",
    "                all_players.add(name)\n",
    "                if c.get('teamId', -1) < AI_TEAM_ID_START:\n",
    "                    human_players.add(name)\n",
    "    \n",
    "    if not all_players or (1 - len(human_players)/len(all_players)) > MAX_AI_RATIO:\n",
    "        return None\n",
    "\n",
    "    # --- Step 3: ì´ˆê¸°í™” (v1ê³¼ ë™ì¼) ---\n",
    "    current_alive = 100\n",
    "    team_players = {}\n",
    "    team_ranks = {}\n",
    "    p_status = {}        # {name: {hp, dbno, veh, loc, last_update}}\n",
    "    p_gear = {}          # {name: {helm, vest}}\n",
    "    p_throw = {}         # {name: {smoke, nade}}\n",
    "    p_items = {}         # [NEW] {name: {medkit, firstaid, painkiller, drink, adrenaline, grenade, flash, molotov}}\n",
    "    p_kills = {}         # {name: kill_count}\n",
    "    curr_safe_zone = {'x': 400000, 'y': 400000}\n",
    "    \n",
    "    next_snap = 0\n",
    "    recording = False\n",
    "    target_teams = set()\n",
    "    data_rows = []\n",
    "\n",
    "    # --- Step 4: íŒ€ êµ¬ì„± ë° ìˆœìœ„ íŒŒì•… ---\n",
    "    for log in logs:\n",
    "        if log.get('_T') == 'LogPlayerPosition':\n",
    "            c = log.get('character') or {}\n",
    "            tid = c.get('teamId', -1)\n",
    "            name = c.get('name')\n",
    "            if tid != -1 and tid < AI_TEAM_ID_START and name:\n",
    "                if tid not in team_players:\n",
    "                    team_players[tid] = set()\n",
    "                team_players[tid].add(name)\n",
    "                if c.get('ranking', 0) != 0:\n",
    "                    team_ranks[tid] = c.get('ranking')\n",
    "\n",
    "    # --- Step 5: ë¡œê·¸ ìˆœì°¨ ì²˜ë¦¬ ---\n",
    "    for log in logs:\n",
    "        log_type = log.get('_T')\n",
    "        elapsed = log.get('elapsedTime', 0)\n",
    "        \n",
    "        # [A] ê²Œì„ ìƒíƒœ\n",
    "        if log_type == 'LogGameStatePeriodic':\n",
    "            sz = (log.get('gameState') or {}).get('safetyZonePosition') or {}\n",
    "            if sz:\n",
    "                curr_safe_zone = {'x': sz.get('x', 400000), 'y': sz.get('y', 400000)}\n",
    "\n",
    "        # [B] í”Œë ˆì´ì–´ ìœ„ì¹˜/ìƒíƒœ (v1ê³¼ ë™ì¼)\n",
    "        elif log_type == 'LogPlayerPosition':\n",
    "            if 'numAlivePlayers' in log:\n",
    "                current_alive = log['numAlivePlayers']\n",
    "            c = log.get('character') or {}\n",
    "            name = c.get('name')\n",
    "            if name:\n",
    "                p_status[name] = {\n",
    "                    'hp': c.get('health', 0),\n",
    "                    'dbno': 1 if c.get('isDBNO', False) else 0,\n",
    "                    'veh': 1 if c.get('isInVehicle', False) else 0,\n",
    "                    'loc': c.get('location') or {},\n",
    "                    'last_update': elapsed\n",
    "                }\n",
    "\n",
    "        # [C] ì¥ë¹„ ì°©ìš© (v1ê³¼ ë™ì¼)\n",
    "        elif log_type == 'LogItemEquip':\n",
    "            item = log.get('item') or {}\n",
    "            name = (log.get('character') or {}).get('name')\n",
    "            if name:\n",
    "                eq_type, level = get_armor_info(item.get('itemId', ''))\n",
    "                if eq_type and level > 0:\n",
    "                    if name not in p_gear:\n",
    "                        p_gear[name] = {'helm': 0, 'vest': 0}\n",
    "                    p_gear[name][eq_type] = level\n",
    "\n",
    "        # [D] ì•„ì´í…œ í”½ì—… (v1 + í™•ì¥)\n",
    "        elif log_type == 'LogItemPickup':\n",
    "            item = log.get('item') or {}\n",
    "            name = (log.get('character') or {}).get('name')\n",
    "            item_id = item.get('itemId', '')\n",
    "            count = item.get('stackCount', 1)\n",
    "            \n",
    "            if name:\n",
    "                # v1: íˆ¬ì²™ ì•„ì´í…œ\n",
    "                if 'SmokeBomb' in item_id:\n",
    "                    if name not in p_throw:\n",
    "                        p_throw[name] = {'smoke': 0, 'nade': 0}\n",
    "                    p_throw[name]['smoke'] += count\n",
    "                elif 'Grenade' in item_id and 'Bluezone' not in item_id:\n",
    "                    if name not in p_throw:\n",
    "                        p_throw[name] = {'smoke': 0, 'nade': 0}\n",
    "                    p_throw[name]['nade'] += count\n",
    "                \n",
    "                # v2: ì¶”ê°€ ì•„ì´í…œ\n",
    "                key = get_item_key(item_id)\n",
    "                if key and key != 'smoke':  # smokeëŠ” p_throwì—ì„œ ê´€ë¦¬\n",
    "                    if name not in p_items:\n",
    "                        p_items[name] = {k: 0 for k in ['medkit', 'firstaid', 'painkiller', 'drink', 'adrenaline', 'grenade', 'flash', 'molotov']}\n",
    "                    p_items[name][key] += count\n",
    "\n",
    "        # [E] ì•„ì´í…œ ë“œë¡­ (v2 ì „ìš©)\n",
    "        elif log_type == 'LogItemDrop':\n",
    "            item = log.get('item') or {}\n",
    "            name = (log.get('character') or {}).get('name')\n",
    "            item_id = item.get('itemId', '')\n",
    "            count = item.get('stackCount', 1)\n",
    "            key = get_item_key(item_id)\n",
    "            if name and key and key != 'smoke' and name in p_items:  # smokeëŠ” p_throwì—ì„œ ê´€ë¦¬\n",
    "                p_items[name][key] = max(0, p_items[name][key] - count)\n",
    "\n",
    "        # [F] ì•„ì´í…œ ì‚¬ìš© (v2 ì „ìš©)\n",
    "        elif log_type == 'LogItemUse':\n",
    "            item = log.get('item') or {}\n",
    "            name = (log.get('character') or {}).get('name')\n",
    "            item_id = item.get('itemId', '')\n",
    "            key = get_item_key(item_id)\n",
    "            if name and key and key != 'smoke' and name in p_items:  # smokeëŠ” p_throwì—ì„œ ê´€ë¦¬\n",
    "                p_items[name][key] = max(0, p_items[name][key] - 1)\n",
    "\n",
    "        # [G] íˆ¬ì²™ ì•„ì´í…œ ì‚¬ìš© (v2 ì „ìš©)\n",
    "        elif log_type == 'LogPlayerUseThrowable':\n",
    "            name = (log.get('character') or {}).get('name')\n",
    "            item_id = log.get('weaponId') or log.get('itemId') or ''\n",
    "            key = get_item_key(item_id)\n",
    "            if name and key and key != 'smoke' and name in p_items:  # smokeëŠ” p_throwì—ì„œ ê´€ë¦¬\n",
    "                p_items[name][key] = max(0, p_items[name][key] - 1)\n",
    "\n",
    "        # [H] í‚¬ ì´ë²¤íŠ¸ (v1ê³¼ ë™ì¼)\n",
    "        elif log_type == 'LogPlayerKillV2':\n",
    "            k = log.get('killer') or {}\n",
    "            v = log.get('victim') or {}\n",
    "            if v and v.get('name') in p_status:\n",
    "                p_status[v['name']]['hp'] = 0\n",
    "            if k and k.get('name'):\n",
    "                kname = k['name']\n",
    "                p_kills[kname] = p_kills.get(kname, 0) + 1\n",
    "\n",
    "        # --- Step 6: ê¸°ë¡ íŠ¸ë¦¬ê±° (v1ê³¼ ë™ì¼) ---\n",
    "        if not recording:\n",
    "            if elapsed > 300 and current_alive <= START_ALIVE_THRESHOLD:\n",
    "                recording = True\n",
    "                next_snap = elapsed\n",
    "                for tid, mems in team_players.items():\n",
    "                    alive = False\n",
    "                    for m in mems:\n",
    "                        s = p_status.get(m, {})\n",
    "                        if s.get('hp', 0) > 0 and (elapsed - s.get('last_update', 0) <= STALE_THRESHOLD_SEC):\n",
    "                            alive = True\n",
    "                            break\n",
    "                    if alive:\n",
    "                        target_teams.add(tid)\n",
    "\n",
    "        # --- Step 7: ìŠ¤ëƒ…ìƒ· ê¸°ë¡ (v1 + í™•ì¥) ---\n",
    "        if recording and elapsed >= next_snap:\n",
    "            # Z ì •ê·œí™” (v1ê³¼ ë™ì¼)\n",
    "            all_z = []\n",
    "            for tid in target_teams:\n",
    "                for m in team_players[tid]:\n",
    "                    s = p_status.get(m, {})\n",
    "                    if s.get('hp', 0) > 0 and s.get('loc'):\n",
    "                        all_z.append(s['loc'].get('z', 0))\n",
    "            z_mean = np.mean(all_z) if all_z else 0\n",
    "            z_std = np.std(all_z) + 1e-5 if all_z else 1\n",
    "\n",
    "            for tid in target_teams:\n",
    "                mems = list(team_players[tid])\n",
    "                team_row = {\n",
    "                    'match_id': os.path.basename(file_path),\n",
    "                    'time_sec': next_snap,\n",
    "                    'team_id': tid,\n",
    "                    'is_winner': 1 if team_ranks.get(tid, 100) == 1 else 0\n",
    "                }\n",
    "                \n",
    "                alive_cnt = 0\n",
    "                total_hp = 0\n",
    "                \n",
    "                for i in range(4):\n",
    "                    prefix = f\"P{i+1}_\"\n",
    "                    if i < len(mems):\n",
    "                        m = mems[i]\n",
    "                        s = p_status.get(m, {})\n",
    "                        g = p_gear.get(m, {'helm': 0, 'vest': 0})\n",
    "                        t = p_throw.get(m, {'smoke': 0, 'nade': 0})\n",
    "                        items = p_items.get(m, {k: 0 for k in ['medkit', 'firstaid', 'painkiller', 'drink', 'adrenaline', 'grenade', 'flash', 'molotov']})\n",
    "                        k = p_kills.get(m, 0)\n",
    "                        \n",
    "                        # v1 ë°©ì‹: ìŠ¤ëƒ…ìƒ· ì‹œì ì— ìƒì¡´ íŒë‹¨\n",
    "                        if s.get('hp', 0) > 0 and (elapsed - s.get('last_update', 0) <= STALE_THRESHOLD_SEC):\n",
    "                            alive_cnt += 1\n",
    "                            total_hp += s['hp']\n",
    "                            loc = s.get('loc', {'x':0, 'y':0, 'z':0})\n",
    "                            \n",
    "                            # v1 ë³€ìˆ˜\n",
    "                            team_row[f'{prefix}hp'] = s['hp']\n",
    "                            team_row[f'{prefix}groggy'] = s['dbno']\n",
    "                            team_row[f'{prefix}helm'] = g['helm']\n",
    "                            team_row[f'{prefix}vest'] = g['vest']\n",
    "                            team_row[f'{prefix}smoke'] = t.get('smoke', 0)\n",
    "                            team_row[f'{prefix}kill'] = k\n",
    "                            \n",
    "                            # v2 ì¶”ê°€ ë³€ìˆ˜\n",
    "                            team_row[f'{prefix}medkit'] = items.get('medkit', 0)\n",
    "                            team_row[f'{prefix}firstaid'] = items.get('firstaid', 0)\n",
    "                            team_row[f'{prefix}painkiller'] = items.get('painkiller', 0)\n",
    "                            team_row[f'{prefix}drink'] = items.get('drink', 0)\n",
    "                            team_row[f'{prefix}adrenaline'] = items.get('adrenaline', 0)\n",
    "                            team_row[f'{prefix}grenade'] = items.get('grenade', 0)\n",
    "                            team_row[f'{prefix}flash'] = items.get('flash', 0)\n",
    "                            team_row[f'{prefix}molotov'] = items.get('molotov', 0)\n",
    "                            \n",
    "                            # ìœ„ì¹˜\n",
    "                            team_row[f'{prefix}dX'] = loc.get('x', 0) - curr_safe_zone['x']\n",
    "                            team_row[f'{prefix}dY'] = loc.get('y', 0) - curr_safe_zone['y']\n",
    "                            team_row[f'{prefix}z_norm'] = (loc.get('z', 0) - z_mean) / z_std\n",
    "                        else:\n",
    "                            # ì£½ì€ í”Œë ˆì´ì–´ = ëª¨ë“  ê°’ 0\n",
    "                            for feat in ['hp','groggy','medkit','firstaid','painkiller','drink','adrenaline',\n",
    "                                        'smoke','grenade','flash','molotov','helm','vest','kill','dX','dY','z_norm']:\n",
    "                                team_row[f'{prefix}{feat}'] = 0\n",
    "                    else:\n",
    "                        # ë¹ˆ ìŠ¬ë¡¯ = ëª¨ë“  ê°’ 0\n",
    "                        for feat in ['hp','groggy','medkit','firstaid','painkiller','drink','adrenaline',\n",
    "                                    'smoke','grenade','flash','molotov','helm','vest','kill','dX','dY','z_norm']:\n",
    "                            team_row[f'{prefix}{feat}'] = 0\n",
    "\n",
    "                team_row['num_alive'] = alive_cnt\n",
    "                team_row['total_health'] = round(total_hp, 2)\n",
    "                data_rows.append(team_row)\n",
    "                \n",
    "            next_snap += TIME_INTERVAL_SEC\n",
    "\n",
    "    # --- Step 8: ë°˜í™˜ ---\n",
    "    return pd.DataFrame(data_rows).round(2) if data_rows else None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "516ae78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ í´ë” ìŠ¤ìº”: C:/Users/qkrwl/Desktop/telemetry_for_top15\n",
      "ğŸ“„ ì²˜ë¦¬ ëŒ€ìƒ íŒŒì¼: ì´ 3848ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ì „ì²´ ì§„í–‰ë¥ : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3848/3848 [14:46<00:00,  4.34file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "ğŸ‰ ì „ì²´ ì‘ì—… ì™„ë£Œ! (886.7ì´ˆ ì†Œìš”)\n",
      "âœ… ì„±ê³µ: 2245\n",
      "â­ï¸ ìŠ¤í‚µ: 1603\n",
      "âŒ ì—ëŸ¬: 0\n",
      "ğŸ“‚ ìµœì¢… íŒŒì¼: data_v3.csv\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# [4] ì‹¤í–‰ë¶€ (ëŒ€ìš©ëŸ‰/ë‹¤ì¤‘ íŒŒì¼ ì•ˆì „ ì²˜ë¦¬ ë²„ì „)\n",
    "# ==========================================\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. íŒŒì¼ ëª©ë¡ ì°¾ê¸°\n",
    "    patterns = [\"*squad_Baltic_Main.jsonl\", \"*squad_Tiger_Main.jsonl\", \"*squad_Desert_Main.jsonl\"]\n",
    "    all_files = []\n",
    "    for pattern in patterns:\n",
    "        all_files.extend(glob.glob(os.path.join(LOG_FOLDER_PATH, pattern)))\n",
    "    total_files = len(all_files)\n",
    "    \n",
    "    print(f\"ğŸ“‚ í´ë” ìŠ¤ìº”: {LOG_FOLDER_PATH}\")\n",
    "    print(f\"ğŸ“„ ì²˜ë¦¬ ëŒ€ìƒ íŒŒì¼: ì´ {total_files}ê°œ\")\n",
    "    \n",
    "    # 2. ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •\n",
    "    BATCH_SIZE = 10  # 10ê°œ íŒŒì¼ë§ˆë‹¤ CSVì— ê¸°ë¡ (ë©”ëª¨ë¦¬ ê´€ë¦¬)\n",
    "    df_buffer = []   # ì„ì‹œ ì €ì¥ì†Œ\n",
    "    \n",
    "    success_cnt = 0\n",
    "    skip_cnt = 0\n",
    "    error_cnt = 0\n",
    "    \n",
    "    # ê¸°ì¡´ íŒŒì¼ì´ ìˆë‹¤ë©´ ì‚­ì œ (ìƒˆë¡œ ì‹œì‘)\n",
    "    if os.path.exists(OUTPUT_CSV):\n",
    "        try:\n",
    "            os.remove(OUTPUT_CSV)\n",
    "            print(f\"ğŸ—‘ï¸ ê¸°ì¡´ ê²°ê³¼ íŒŒì¼ ì‚­ì œë¨: {OUTPUT_CSV}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ê¸°ì¡´ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ (ì—´ë ¤ìˆì„ ìˆ˜ ìˆìŒ): {e}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "# 3. ë°˜ë³µ ì²˜ë¦¬ ë£¨í”„ (tqdm ì ìš©)\n",
    "    # desc: ì§„í–‰ë°” ì™¼ìª½ì— í‘œì‹œë  í…ìŠ¤íŠ¸, unit: ë‹¨ìœ„\n",
    "    for i, file_path in enumerate(tqdm(all_files, desc=\"ì „ì²´ ì§„í–‰ë¥ \", unit=\"file\")):\n",
    "        current_idx = i + 1\n",
    "        fname = os.path.basename(file_path)\n",
    "        \n",
    "        try:\n",
    "            # ê°œë³„ íŒŒì¼ ì²˜ë¦¬ ì‹œë„\n",
    "            # (ì°¸ê³ : process_match_log í•¨ìˆ˜ ë‚´ì˜ printë¬¸ì€ ì£¼ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ tqdm.writeë¥¼ ì“°ëŠ” ê²Œ ê¹”ë”í•©ë‹ˆë‹¤)\n",
    "            df = process_match_log(file_path)\n",
    "            \n",
    "            if df is not None and not df.empty:\n",
    "                df_buffer.append(df)\n",
    "                success_cnt += 1\n",
    "            else:\n",
    "                skip_cnt += 1\n",
    "                # tqdm ì‚¬ìš© ì‹œ print ëŒ€ì‹  tqdm.writeë¥¼ ì¨ì•¼ ì§„í–‰ë°”ê°€ ê¹¨ì§€ì§€ ì•ŠìŒ\n",
    "                # tqdm.write(f\"  â”” [ìŠ¤í‚µ] {fname}\") \n",
    "\n",
    "        except Exception as e:\n",
    "            error_cnt += 1\n",
    "            tqdm.write(f\"  â”” [ì—ëŸ¬] {fname}: {str(e)}\") # ì—ëŸ¬ë§Œ ì¶œë ¥\n",
    "            continue \n",
    "\n",
    "        # 4. ë°°ì¹˜ ë‹¨ìœ„ ì €ì¥ (ë˜ëŠ” ë§ˆì§€ë§‰ íŒŒì¼ì¸ ê²½ìš°)\n",
    "        if len(df_buffer) >= BATCH_SIZE or (current_idx == total_files and df_buffer):\n",
    "            # ë²„í¼ í•©ì¹˜ê¸°\n",
    "            batch_df = pd.concat(df_buffer, ignore_index=True)\n",
    "            \n",
    "            # CSV ì €ì¥ (ì²« ì €ì¥ì´ë©´ í—¤ë” í¬í•¨, ì•„ë‹ˆë©´ ì´ì–´ì“°ê¸°)\n",
    "            if not os.path.exists(OUTPUT_CSV):\n",
    "                batch_df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig', mode='w')\n",
    "            else:\n",
    "                batch_df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig', mode='a', header=False)\n",
    "            \n",
    "            # ë²„í¼ ë¹„ìš°ê¸°\n",
    "            df_buffer = []\n",
    "            \n",
    "            # ì§„í–‰ ìƒí™© ì‚´ì§ ë³´ì—¬ì£¼ê¸° (ì„ íƒ ì‚¬í•­)\n",
    "            # tqdm.write(f\"ğŸ’¾ {current_idx}ê°œ íŒŒì¼ ì²˜ë¦¬ í›„ ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "    # 5. ìµœì¢… ë¦¬í¬íŠ¸\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"ğŸ‰ ì „ì²´ ì‘ì—… ì™„ë£Œ! ({elapsed:.1f}ì´ˆ ì†Œìš”)\")\n",
    "    print(f\"âœ… ì„±ê³µ: {success_cnt}\")\n",
    "    print(f\"â­ï¸ ìŠ¤í‚µ: {skip_cnt}\")\n",
    "    print(f\"âŒ ì—ëŸ¬: {error_cnt}\")\n",
    "    \n",
    "    if os.path.exists(OUTPUT_CSV):\n",
    "        print(f\"ğŸ“‚ ìµœì¢… íŒŒì¼: {OUTPUT_CSV}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ ì €ì¥ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00e8cf2",
   "metadata": {},
   "source": [
    "# **2. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ êµ¬ì¶• ë° ëª¨ë¸ í•™ìŠµ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748587e5",
   "metadata": {},
   "source": [
    "## 2-1. íŒ€ì› ê°„ ìƒí˜¸ì‘ìš© Transformer + íŒ€ ì •ë³´ ë‹¨ìˆœ Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7d828445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ í•™ìŠµ ì¥ì¹˜: cuda\n",
      "ğŸ“Š í”Œë ˆì´ì–´ ë³€ìˆ˜: 17ê°œ / íŒ€ ë³€ìˆ˜: 2ê°œ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# [1] ì„¤ì • (Configuration)\n",
    "# ==========================================\n",
    "# â˜… ì¤‘ìš”: ë°©ê¸ˆ ë§Œë“  ë°ì´í„°ì…‹ íŒŒì¼ëª…ìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš”\n",
    "DATA_FILE = 'data_v3.csv' \n",
    "MODEL_PATH = 'pubg_win_model_v2_datav3.pth'       \n",
    "\n",
    "# ëª¨ë¸ íŒŒë¼ë¯¸í„°\n",
    "MAX_TEAMS = 10       # Top 10 ì˜ˆì¸¡ì´ë¯€ë¡œ ìµœëŒ€ 10íŒ€\n",
    "MAX_PLAYERS = 4      # ìŠ¤ì¿¼ë“œ ìµœëŒ€ 4ëª…\n",
    "\n",
    "# â˜… ë³€ê²½ëœ ë³€ìˆ˜ êµ¬ì„± (ì´ 17ê°œ)\n",
    "P_COLS = [\n",
    "    'hp', 'groggy',                        # [ìƒíƒœ] 2ê°œ\n",
    "    'medkit', 'firstaid', 'painkiller', 'drink', 'adrenaline', # [í] 5ê°œ\n",
    "    'smoke', 'grenade', 'flash', 'molotov', # [íˆ¬ì²™] 4ê°œ\n",
    "    'helm', 'vest',                        # [ì¥ë¹„] 2ê°œ\n",
    "    'kill',                                # [í‚¬] 1ê°œ\n",
    "    'dX', 'dY', 'z_norm'                   # [ìœ„ì¹˜] 3ê°œ\n",
    "]\n",
    "\n",
    "T_COLS = ['num_alive', 'total_health']     # [íŒ€ ë³€ìˆ˜]\n",
    "\n",
    "PLAYER_FEATS = len(P_COLS) # 17\n",
    "TEAM_FEATS = len(T_COLS)   # 2\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "HIDDEN_DIM = 32\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "LR = 0.0005\n",
    "DROPOUT = 0.4        # [ìˆ˜ì •] Dropout ë¹„ìœ¨ ìƒí–¥ (0.3 -> 0.4)\n",
    "WEIGHT_DECAY = 1e-5  # [ì¶”ê°€] L2 Regularization (ê°€ì¤‘ì¹˜ ê°ì‡ )\n",
    "PATIENCE = 7        # [ì¶”ê°€] ì„±ëŠ¥ í–¥ìƒ ì—†ì„ ì‹œ ê¸°ë‹¤ë¦´ Epoch ìˆ˜\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"ğŸ”¥ í•™ìŠµ ì¥ì¹˜: {device}\")\n",
    "print(f\"ğŸ“Š í”Œë ˆì´ì–´ ë³€ìˆ˜: {PLAYER_FEATS}ê°œ / íŒ€ ë³€ìˆ˜: {TEAM_FEATS}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f1cfab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# [2] ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (ì „ì²˜ë¦¬)\n",
    "# ==========================================\n",
    "class PUBGDataset(Dataset):\n",
    "    def __init__(self, df, p_scaler=None, t_scaler=None, is_train=True):\n",
    "        self.samples = [] \n",
    "        self.labels = []\n",
    "        \n",
    "        # 1. ìŠ¤ì¼€ì¼ë§\n",
    "        if is_train:\n",
    "            all_p_data = []\n",
    "            for i in range(1, MAX_PLAYERS + 1):\n",
    "                cols = [f\"P{i}_{c}\" for c in P_COLS]\n",
    "                valid_cols = [c for c in cols if c in df.columns]\n",
    "                if valid_cols:\n",
    "                    all_p_data.append(df[valid_cols].values)\n",
    "            \n",
    "            if all_p_data:\n",
    "                all_p_data = np.vstack(all_p_data)\n",
    "                self.p_scaler = StandardScaler()\n",
    "                self.p_scaler.fit(all_p_data)\n",
    "            else:\n",
    "                self.p_scaler = None\n",
    "            \n",
    "            t_cols_found = [c for c in T_COLS if c in df.columns]\n",
    "            if t_cols_found:\n",
    "                self.t_scaler = StandardScaler()\n",
    "                self.t_scaler.fit(df[t_cols_found].values)\n",
    "            else:\n",
    "                self.t_scaler = None\n",
    "        else:\n",
    "            self.p_scaler = p_scaler\n",
    "            self.t_scaler = t_scaler\n",
    "            \n",
    "        # 2. í…ì„œ ë³€í™˜\n",
    "        feature_groups = []\n",
    "        for i in range(1, MAX_PLAYERS + 1):\n",
    "            cols = [f\"P{i}_{c}\" for c in P_COLS]\n",
    "            p_data = df[cols].fillna(0).values if all(c in df.columns for c in cols) else np.zeros((len(df), len(P_COLS)))\n",
    "            feature_groups.append(p_data)\n",
    "            \n",
    "        self.p_features = np.stack(feature_groups, axis=1) \n",
    "        \n",
    "        if self.p_scaler:\n",
    "            N, P, F = self.p_features.shape\n",
    "            self.p_features = self.p_scaler.transform(self.p_features.reshape(-1, F)).reshape(N, P, F)\n",
    "\n",
    "        self.t_features = df[T_COLS].fillna(0).values\n",
    "        if self.t_scaler:\n",
    "            self.t_features = self.t_scaler.transform(self.t_features)\n",
    "            \n",
    "        self.labels = df['is_winner'].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        p_feat = torch.FloatTensor(self.p_features[idx]) \n",
    "        t_feat = torch.FloatTensor(self.t_features[idx]) \n",
    "        label = torch.FloatTensor([self.labels[idx]])    \n",
    "        return p_feat, t_feat, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5406e52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# [3] ëª¨ë¸ (Hierarchical Transformer)\n",
    "# ==========================================\n",
    "class WinPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Level 1: Player Encoder\n",
    "        self.p_embed = nn.Sequential(\n",
    "            nn.Linear(PLAYER_FEATS, HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT)\n",
    "        )\n",
    "        \n",
    "        self.p_enc = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=HIDDEN_DIM, nhead=4, dim_feedforward=128, batch_first=True, dropout=DROPOUT),\n",
    "            num_layers=2\n",
    "        )\n",
    "        \n",
    "        # Level 2: Team Aggregation\n",
    "        self.t_embed = nn.Sequential(\n",
    "            nn.Linear(TEAM_FEATS, HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT)\n",
    "        )\n",
    "        \n",
    "        # Final Prediction\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(HIDDEN_DIM * 2, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, p_x, t_x):\n",
    "        p_emb = self.p_embed(p_x)      \n",
    "        p_out = self.p_enc(p_emb)      \n",
    "        p_summary = torch.max(p_out, dim=1)[0] \n",
    "        \n",
    "        t_emb = self.t_embed(t_x)      \n",
    "        \n",
    "        combined = torch.cat([p_summary, t_emb], dim=1) \n",
    "        output = self.classifier(combined)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aac5d786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: 184744í–‰\n",
      "ğŸ”¹ í•™ìŠµ ë§¤ì¹˜: 1796ê°œ | ê²€ì¦ ë§¤ì¹˜: 449ê°œ\n",
      "\n",
      "ğŸš€ í•™ìŠµ ì‹œì‘...\n",
      "\n",
      "ğŸš€ í•™ìŠµ ì‹œì‘...\n",
      "Epoch 01 | Loss: 0.3858 | Train Acc: 80.91% | Val Acc: 80.75% \n",
      "  ğŸ’¾ ëª¨ë¸ ì €ì¥ë¨ (Best Val Acc: 80.75%)\n",
      "Epoch 01 | Loss: 0.3858 | Train Acc: 80.91% | Val Acc: 80.75% \n",
      "  ğŸ’¾ ëª¨ë¸ ì €ì¥ë¨ (Best Val Acc: 80.75%)\n",
      "Epoch 02 | Loss: 0.3731 | Train Acc: 81.59% | Val Acc: 80.78% \n",
      "  ğŸ’¾ ëª¨ë¸ ì €ì¥ë¨ (Best Val Acc: 80.78%)\n",
      "Epoch 02 | Loss: 0.3731 | Train Acc: 81.59% | Val Acc: 80.78% \n",
      "  ğŸ’¾ ëª¨ë¸ ì €ì¥ë¨ (Best Val Acc: 80.78%)\n",
      "Epoch 03 | Loss: 0.3684 | Train Acc: 81.87% | Val Acc: 80.76% \n",
      "Epoch 03 | Loss: 0.3684 | Train Acc: 81.87% | Val Acc: 80.76% \n",
      "Epoch 04 | Loss: 0.3653 | Train Acc: 82.03% | Val Acc: 80.81% \n",
      "  ğŸ’¾ ëª¨ë¸ ì €ì¥ë¨ (Best Val Acc: 80.81%)\n",
      "Epoch 04 | Loss: 0.3653 | Train Acc: 82.03% | Val Acc: 80.81% \n",
      "  ğŸ’¾ ëª¨ë¸ ì €ì¥ë¨ (Best Val Acc: 80.81%)\n",
      "Epoch 05 | Loss: 0.3632 | Train Acc: 82.16% | Val Acc: 80.50% \n",
      "Epoch 05 | Loss: 0.3632 | Train Acc: 82.16% | Val Acc: 80.50% \n",
      "Epoch 06 | Loss: 0.3618 | Train Acc: 82.21% | Val Acc: 80.83% \n",
      "  ğŸ’¾ ëª¨ë¸ ì €ì¥ë¨ (Best Val Acc: 80.83%)\n",
      "Epoch 06 | Loss: 0.3618 | Train Acc: 82.21% | Val Acc: 80.83% \n",
      "  ğŸ’¾ ëª¨ë¸ ì €ì¥ë¨ (Best Val Acc: 80.83%)\n",
      "Epoch 07 | Loss: 0.3604 | Train Acc: 82.41% | Val Acc: 80.59% \n",
      "Epoch 07 | Loss: 0.3604 | Train Acc: 82.41% | Val Acc: 80.59% \n",
      "Epoch 08 | Loss: 0.3583 | Train Acc: 82.51% | Val Acc: 80.62% \n",
      "Epoch 08 | Loss: 0.3583 | Train Acc: 82.51% | Val Acc: 80.62% \n",
      "Epoch 09 | Loss: 0.3577 | Train Acc: 82.57% | Val Acc: 80.71% \n",
      "Epoch 09 | Loss: 0.3577 | Train Acc: 82.57% | Val Acc: 80.71% \n",
      "Epoch 10 | Loss: 0.3567 | Train Acc: 82.58% | Val Acc: 80.65% \n",
      "Epoch 10 | Loss: 0.3567 | Train Acc: 82.58% | Val Acc: 80.65% \n",
      "Epoch 11 | Loss: 0.3557 | Train Acc: 82.62% | Val Acc: 80.63% \n",
      "Epoch 11 | Loss: 0.3557 | Train Acc: 82.62% | Val Acc: 80.63% \n",
      "Epoch 12 | Loss: 0.3542 | Train Acc: 82.59% | Val Acc: 80.71%  | ğŸ”» LR ê°ì†Œ: 0.0005 -> 0.00025\n",
      "Epoch 12 | Loss: 0.3542 | Train Acc: 82.59% | Val Acc: 80.71%  | ğŸ”» LR ê°ì†Œ: 0.0005 -> 0.00025\n",
      "Epoch 13 | Loss: 0.3525 | Train Acc: 82.80% | Val Acc: 80.52% \n",
      "\n",
      "â¹ï¸ Early Stopping ë°œë™! (Epoch 13 ì¢…ë£Œ)\n",
      "\n",
      "ğŸ† ìµœì¢… ì™„ë£Œ! ìµœê³  ê²€ì¦ ì •í™•ë„: 80.83%\n",
      "Epoch 13 | Loss: 0.3525 | Train Acc: 82.80% | Val Acc: 80.52% \n",
      "\n",
      "â¹ï¸ Early Stopping ë°œë™! (Epoch 13 ì¢…ë£Œ)\n",
      "\n",
      "ğŸ† ìµœì¢… ì™„ë£Œ! ìµœê³  ê²€ì¦ ì •í™•ë„: 80.83%\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# [4] í•™ìŠµ ë£¨í”„ (Train Acc ì¶”ê°€ & ëª¨ë‹ˆí„°ë§ ê°•í™”)\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(DATA_FILE):\n",
    "        print(f\"âŒ ë°ì´í„° íŒŒì¼({DATA_FILE})ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        # ë°ì´í„° ë¡œë“œ\n",
    "        df = pd.read_csv(DATA_FILE)\n",
    "        print(f\"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}í–‰\")\n",
    "        \n",
    "        if len(df) < 100:\n",
    "            print(\"âš ï¸ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤.\")\n",
    "        else:\n",
    "            match_ids = df['match_id'].unique()\n",
    "            train_ids, test_ids = train_test_split(match_ids, test_size=0.2, random_state=42)\n",
    "            \n",
    "            train_df = df[df['match_id'].isin(train_ids)]\n",
    "            test_df = df[df['match_id'].isin(test_ids)]\n",
    "            \n",
    "            print(f\"ğŸ”¹ í•™ìŠµ ë§¤ì¹˜: {len(train_ids)}ê°œ | ê²€ì¦ ë§¤ì¹˜: {len(test_ids)}ê°œ\")\n",
    "            \n",
    "            train_ds = PUBGDataset(train_df, is_train=True)\n",
    "            test_ds = PUBGDataset(test_df, p_scaler=train_ds.p_scaler, t_scaler=train_ds.t_scaler, is_train=False)\n",
    "            \n",
    "            train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "            test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            \n",
    "            model = WinPredictor().to(device)\n",
    "            criterion = nn.BCELoss()\n",
    "            \n",
    "            optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "            \n",
    "            print(\"\\nğŸš€ í•™ìŠµ ì‹œì‘...\")\n",
    "            best_acc = 0\n",
    "            patience_count = 0 \n",
    "            \n",
    "            for epoch in range(EPOCHS):\n",
    "                # --- [Training] ---\n",
    "                model.train()\n",
    "                train_loss = 0\n",
    "                train_correct = 0\n",
    "                train_total = 0\n",
    "                \n",
    "                for p_x, t_x, y in train_loader:\n",
    "                    p_x, t_x, y = p_x.to(device), t_x.to(device), y.to(device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    pred = model(p_x, t_x)\n",
    "                    loss = criterion(pred, y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    train_loss += loss.item()\n",
    "                    \n",
    "                    # Train Acc ê³„ì‚°\n",
    "                    predicted = (pred > 0.5).float()\n",
    "                    train_correct += (predicted == y).sum().item()\n",
    "                    train_total += y.size(0)\n",
    "                \n",
    "                avg_train_loss = train_loss / len(train_loader)\n",
    "                train_acc = 100 * train_correct / train_total\n",
    "                \n",
    "                # --- [Validation] ---\n",
    "                model.eval()\n",
    "                val_correct = 0\n",
    "                val_total = 0\n",
    "                with torch.no_grad():\n",
    "                    for p_x, t_x, y in test_loader:\n",
    "                        p_x, t_x, y = p_x.to(device), t_x.to(device), y.to(device)\n",
    "                        pred = model(p_x, t_x)\n",
    "                        predicted = (pred > 0.5).float()\n",
    "                        val_correct += (predicted == y).sum().item()\n",
    "                        val_total += y.size(0)\n",
    "                \n",
    "                val_acc = 100 * val_correct / val_total\n",
    "                \n",
    "                # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸\n",
    "                prev_lr = optimizer.param_groups[0]['lr']\n",
    "                scheduler.step(val_acc)\n",
    "                curr_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "                lr_msg = \"\"\n",
    "                if curr_lr != prev_lr:\n",
    "                    lr_msg = f\" | ğŸ”» LR ê°ì†Œ: {prev_lr} -> {curr_lr}\"\n",
    "\n",
    "                # ê²°ê³¼ ì¶œë ¥ (Train Acc ì¶”ê°€ë¨)\n",
    "                print(f\"Epoch {epoch+1:02d} | Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}% {lr_msg}\")\n",
    "                \n",
    "                # Early Stopping\n",
    "                if val_acc > best_acc:\n",
    "                    best_acc = val_acc\n",
    "                    patience_count = 0 \n",
    "                    torch.save({\n",
    "                        'model': model.state_dict(),\n",
    "                        'p_scaler': train_ds.p_scaler,\n",
    "                        't_scaler': train_ds.t_scaler\n",
    "                    }, MODEL_PATH)\n",
    "                    print(f\"  ğŸ’¾ ëª¨ë¸ ì €ì¥ë¨ (Best Val Acc: {best_acc:.2f}%)\")\n",
    "                else:\n",
    "                    patience_count += 1\n",
    "                    if patience_count >= PATIENCE:\n",
    "\n",
    "                        print(f\"\\nâ¹ï¸ Early Stopping ë°œë™! (Epoch {epoch+1} ì¢…ë£Œ)\")            \n",
    "                        print(f\"\\nğŸ† ìµœì¢… ì™„ë£Œ! ìµœê³  ê²€ì¦ ì •í™•ë„: {best_acc:.2f}%\")\n",
    "\n",
    "                        break            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce75aba",
   "metadata": {},
   "source": [
    "## 2-2. íŒ€ì› ì •ë³´ MLP + Sum Pooling + íŒ€ ì •ë³´ MLP + íŒ€ ê°„ Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d91b4671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ í•™ìŠµ ì¥ì¹˜: cuda\n",
      "ğŸ”¹ í•™ìŠµ: 1752ê°œ ë§¤ì¹˜ | ê²€ì¦: 439ê°œ ë§¤ì¹˜\n",
      "\n",
      "ğŸš€ Light Match Transformer í•™ìŠµ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Loss: 0.7130 | Train Acc: 69.01% | Val Acc: 69.19% \n",
      "  ğŸ’¾ Best Model Saved (69.19%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | Loss: 0.6475 | Train Acc: 71.67% | Val Acc: 69.27% \n",
      "  ğŸ’¾ Best Model Saved (69.27%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | Loss: 0.6275 | Train Acc: 72.31% | Val Acc: 68.87% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | Loss: 0.6102 | Train Acc: 73.07% | Val Acc: 67.93% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | Loss: 0.5973 | Train Acc: 73.79% | Val Acc: 68.27% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 | Loss: 0.5871 | Train Acc: 74.08% | Val Acc: 67.43% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07 | Loss: 0.5757 | Train Acc: 74.71% | Val Acc: 68.02% \n",
      "â¹ï¸ Early Stopping (Epoch 7)\n",
      "\n",
      "ğŸ† ìµœì¢… ì™„ë£Œ! ìµœê³  ê²€ì¦ ì •í™•ë„: 69.27%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================================\n",
    "# [1] ì„¤ì • (Configuration)\n",
    "# ==========================================\n",
    "DATA_FILE = 'data_v2_filtered.csv' \n",
    "MODEL_PATH = 'pubg_win_model_v3.pth'\n",
    "\n",
    "# íŒŒë¼ë¯¸í„°\n",
    "MAX_TEAMS = 10       \n",
    "MAX_PLAYERS = 4      \n",
    "\n",
    "# ë³€ìˆ˜ êµ¬ì„± (17ê°œ)\n",
    "P_COLS = [\n",
    "    'hp', 'groggy',                        \n",
    "    'firstaid',\n",
    "    'smoke', 'grenade',\n",
    "    'helm', 'vest',                        \n",
    "    'kill',                                \n",
    "    'dX', 'dY', 'z_norm'                   \n",
    "]\n",
    "T_COLS = ['num_alive', 'total_health']     \n",
    "\n",
    "PLAYER_FEATS = len(P_COLS) # 17\n",
    "TEAM_FEATS = len(T_COLS)   # 2\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° (ëª¨ë¸ì´ ê°€ë²¼ì›Œì¡Œìœ¼ë¯€ë¡œ í•™ìŠµë¥  ì¡°ì • ê°€ëŠ¥)\n",
    "HIDDEN_DIM = 32      # 128 -> 64ë¡œ ì°¨ì› ì¶•ì†Œ (ë” ê°€ë³ê²Œ)\n",
    "BATCH_SIZE = 64     \n",
    "EPOCHS = 30\n",
    "LR = 0.0005 \n",
    "DROPOUT = 0.3\n",
    "PATIENCE = 5        \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸ”¥ í•™ìŠµ ì¥ì¹˜: {device}\")\n",
    "\n",
    "# ==========================================\n",
    "# [2] ë°ì´í„°ì…‹ (Data Augmentation ì¶”ê°€)\n",
    "# ==========================================\n",
    "class PUBGMatchDataset(Dataset):\n",
    "    def __init__(self, df, p_scaler=None, t_scaler=None, is_train=True):\n",
    "        self.is_train = is_train  # í•™ìŠµ ëª¨ë“œì¸ì§€ í™•ì¸\n",
    "        \n",
    "        # 1. ìŠ¤ì¼€ì¼ëŸ¬ ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)\n",
    "        if is_train:\n",
    "            all_p_data = []\n",
    "            for i in range(1, MAX_PLAYERS + 1):\n",
    "                cols = [f\"P{i}_{c}\" for c in P_COLS]\n",
    "                valid_cols = [c for c in cols if c in df.columns]\n",
    "                if valid_cols: all_p_data.append(df[valid_cols].values)\n",
    "            \n",
    "            self.p_scaler = StandardScaler()\n",
    "            if all_p_data: self.p_scaler.fit(np.vstack(all_p_data))\n",
    "            \n",
    "            self.t_scaler = StandardScaler()\n",
    "            self.t_scaler.fit(df[T_COLS].values)\n",
    "        else:\n",
    "            self.p_scaler = p_scaler\n",
    "            self.t_scaler = t_scaler\n",
    "\n",
    "        self.groups = list(df.groupby(['match_id', 'time_sec']))\n",
    "        self.data_dict = {i: group for i, (key, group) in enumerate(self.groups)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        group_df = self.data_dict[idx]\n",
    "        \n",
    "        # --- ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ë¡œì§ ---\n",
    "        p_feats_list = []\n",
    "        for i in range(1, MAX_PLAYERS + 1):\n",
    "            cols = [f\"P{i}_{c}\" for c in P_COLS]\n",
    "            p_data = group_df[cols].fillna(0).values if all(c in group_df.columns for c in cols) else np.zeros((len(group_df), len(P_COLS)))\n",
    "            p_feats_list.append(p_data)\n",
    "        \n",
    "        p_features = np.stack(p_feats_list, axis=1) \n",
    "        if self.p_scaler:\n",
    "            N, P, F = p_features.shape\n",
    "            p_features = self.p_scaler.transform(p_features.reshape(-1, F)).reshape(N, P, F)\n",
    "\n",
    "        t_features = group_df[T_COLS].fillna(0).values\n",
    "        if self.t_scaler:\n",
    "            t_features = self.t_scaler.transform(t_features)\n",
    "            \n",
    "        winners = group_df['is_winner'].values\n",
    "        # ì‹¤ì œ ë°ì´í„°ì— ìˆëŠ” íŒ€ ìˆ˜\n",
    "        num_actual_teams = len(winners)\n",
    "\n",
    "        # --- Padding ---\n",
    "        curr_teams = p_features.shape[0]\n",
    "        mask = torch.zeros(MAX_TEAMS, dtype=torch.bool)\n",
    "        \n",
    "        # íŒ¨ë”©ì„ ë¯¸ë¦¬ ì±„ì›Œì„œ (10, 4, 17) í˜•íƒœë¡œ ë§Œë“¦\n",
    "        final_p = np.zeros((MAX_TEAMS, MAX_PLAYERS, PLAYER_FEATS))\n",
    "        final_t = np.zeros((MAX_TEAMS, TEAM_FEATS))\n",
    "        final_w = np.zeros((MAX_TEAMS)) # ìš°ìŠ¹ ì—¬ë¶€ (0 or 1)\n",
    "\n",
    "        # ì‹¤ì œ ë°ì´í„° ë³µì‚¬\n",
    "        limit = min(curr_teams, MAX_TEAMS)\n",
    "        final_p[:limit] = p_features[:limit]\n",
    "        final_t[:limit] = t_features[:limit]\n",
    "        final_w[:limit] = winners[:limit]\n",
    "        \n",
    "        # ë§ˆìŠ¤í‚¹ ì²˜ë¦¬ (ë¹ˆ ê³µê°„)\n",
    "        mask[limit:] = True\n",
    "\n",
    "        # â˜…â˜…â˜… [í•µì‹¬] ë°ì´í„° ì¦ê°• (íŒ€ ìˆœì„œ ì„ê¸°) â˜…â˜…â˜…\n",
    "        # í•™ìŠµ(Train) ì¤‘ì¼ ë•Œë§Œ ëœë¤í•˜ê²Œ ì„ìŒ\n",
    "        if self.is_train:\n",
    "            # 0~9ë²ˆ ì¸ë±ìŠ¤ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ìŒ\n",
    "            perm_indices = np.random.permutation(MAX_TEAMS)\n",
    "            \n",
    "            final_p = final_p[perm_indices]\n",
    "            final_t = final_t[perm_indices]\n",
    "            final_w = final_w[perm_indices]\n",
    "            mask = mask[perm_indices]\n",
    "            \n",
    "        # ì„ì¸ í›„ì˜ ìš°ìŠ¹íŒ€ ì¸ë±ìŠ¤ ì°¾ê¸°\n",
    "        if np.sum(final_w) > 0:\n",
    "            target_idx = np.argmax(final_w)\n",
    "        else:\n",
    "            target_idx = -100\n",
    "\n",
    "        return (torch.FloatTensor(final_p), \n",
    "                torch.FloatTensor(final_t), \n",
    "                mask, \n",
    "                torch.tensor(target_idx, dtype=torch.long))\n",
    "# ==========================================\n",
    "# [3] ëª¨ë¸ (Lightweight Match Predictor)\n",
    "# ==========================================\n",
    "class MatchWinPredictor_Light(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Player Embedding (1ì¸µìœ¼ë¡œ ì¶•ì†Œ)\n",
    "        # ë³µì¡í•œ ì¶”ë¡ ë³´ë‹¤ëŠ” 'íŠ¹ì§• ë§¤í•‘'ì— ì§‘ì¤‘\n",
    "        self.p_embed = nn.Sequential(\n",
    "            nn.Linear(PLAYER_FEATS, HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT)\n",
    "        )\n",
    "        \n",
    "        # 2. Team Embedding (1ì¸µ ìœ ì§€)\n",
    "        self.t_embed = nn.Sequential(\n",
    "            nn.Linear(TEAM_FEATS, HIDDEN_DIM),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 3. Match Context (1ì¸µìœ¼ë¡œ ì¶•ì†Œ)\n",
    "        # ì—¬ëŸ¬ ë²ˆ ê¼¬ì•„ì„œ ìƒê°í•˜ì§€ ì•Šê³ , í•œ ë²ˆì˜ ë¹„êµ(Attention)ë¡œ íŒë‹¨\n",
    "        self.match_enc = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=HIDDEN_DIM * 2, nhead=4, dim_feedforward=128, batch_first=True, dropout=DROPOUT),\n",
    "            num_layers=1  # [ë³€ê²½] 2 -> 1 layer\n",
    "        )\n",
    "        \n",
    "        # 4. Output (ìœ ì§€)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(HIDDEN_DIM * 2, 32), # 64 -> 32\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, p_x, t_x, mask):\n",
    "        # p_x: (Batch, 10, 4, 17)\n",
    "        B, T, P, F = p_x.shape\n",
    "        \n",
    "        # [Step 1] Player Feature Extraction\n",
    "        p_flat = p_x.view(B * T * P, F)\n",
    "        p_emb = self.p_embed(p_flat)        # (B*T*P, H)\n",
    "        p_emb = p_emb.view(B * T, P, -1)    # (B*T, 4, H)\n",
    "        \n",
    "        # Sum Pooling (íŒ€ ì „ë ¥ í•©ì‚°)\n",
    "        p_summary = torch.sum(p_emb, dim=1) # (B*T, H)\n",
    "        \n",
    "        # [Step 2] Team Feature Fusion\n",
    "        t_flat = t_x.view(B * T, -1)\n",
    "        t_emb = self.t_embed(t_flat)        # (B*T, H)\n",
    "        \n",
    "        team_vec = torch.cat([p_summary, t_emb], dim=1) # (B*T, H*2)\n",
    "        \n",
    "        # [Step 3] Match Context\n",
    "        team_vec_match = team_vec.view(B, T, -1) # (B, 10, H*2)\n",
    "        match_out = self.match_enc(team_vec_match, src_key_padding_mask=mask) \n",
    "        \n",
    "        # [Step 4] Prediction\n",
    "        logits = self.classifier(match_out).squeeze(-1) # (B, 10)\n",
    "        logits = logits.masked_fill(mask, -1e9)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# ==========================================\n",
    "# [4] í•™ìŠµ ë£¨í”„\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(DATA_FILE):\n",
    "        print(f\"âŒ ë°ì´í„° íŒŒì¼({DATA_FILE}) ì—†ìŒ\")\n",
    "    else:\n",
    "        df = pd.read_csv(DATA_FILE)\n",
    "        \n",
    "        match_ids = df['match_id'].unique()\n",
    "        train_ids, test_ids = train_test_split(match_ids, test_size=0.2, random_state=42)\n",
    "        \n",
    "        train_df = df[df['match_id'].isin(train_ids)]\n",
    "        test_df = df[df['match_id'].isin(test_ids)]\n",
    "        \n",
    "        print(f\"ğŸ”¹ í•™ìŠµ: {len(train_ids)}ê°œ ë§¤ì¹˜ | ê²€ì¦: {len(test_ids)}ê°œ ë§¤ì¹˜\")\n",
    "        \n",
    "        train_ds = PUBGMatchDataset(train_df, is_train=True)\n",
    "        test_ds = PUBGMatchDataset(test_df, p_scaler=train_ds.p_scaler, t_scaler=train_ds.t_scaler, is_train=False)\n",
    "        \n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "        test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        model = MatchWinPredictor_Light().to(device) # Light ëª¨ë¸ ì‚¬ìš©\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=-100) \n",
    "        optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "        \n",
    "        print(\"\\nğŸš€ Light Match Transformer í•™ìŠµ ì‹œì‘...\")\n",
    "        best_acc = 0\n",
    "        patience_count = 0\n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "            # --- Train ---\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            \n",
    "            pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
    "            for p_x, t_x, mask, target in pbar:\n",
    "                p_x, t_x = p_x.to(device), t_x.to(device)\n",
    "                mask, target = mask.to(device), target.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                logits = model(p_x, t_x, mask)\n",
    "                loss = criterion(logits, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                valid = target != -100\n",
    "                train_correct += (preds[valid] == target[valid]).sum().item()\n",
    "                train_total += valid.sum().item()\n",
    "                \n",
    "                pbar.set_postfix({'Loss': loss.item()})\n",
    "            \n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            train_acc = 100 * train_correct / train_total if train_total > 0 else 0\n",
    "            \n",
    "            # --- Val ---\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            with torch.no_grad():\n",
    "                for p_x, t_x, mask, target in test_loader:\n",
    "                    p_x, t_x = p_x.to(device), t_x.to(device)\n",
    "                    mask, target = mask.to(device), target.to(device)\n",
    "                    \n",
    "                    logits = model(p_x, t_x, mask)\n",
    "                    preds = torch.argmax(logits, dim=1)\n",
    "                    \n",
    "                    valid = target != -100\n",
    "                    val_correct += (preds[valid] == target[valid]).sum().item()\n",
    "                    val_total += valid.sum().item()\n",
    "            \n",
    "            val_acc = 100 * val_correct / val_total if val_total > 0 else 0\n",
    "            \n",
    "            prev_lr = optimizer.param_groups[0]['lr']\n",
    "            scheduler.step(val_acc)\n",
    "            curr_lr = optimizer.param_groups[0]['lr']\n",
    "            lr_msg = f\" | ğŸ”» LR: {curr_lr}\" if curr_lr != prev_lr else \"\"\n",
    "            \n",
    "            print(f\"Epoch {epoch+1:02d} | Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}% {lr_msg}\")\n",
    "            \n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                patience_count = 0\n",
    "                torch.save(model.state_dict(), MODEL_PATH)\n",
    "                print(f\"  ğŸ’¾ Best Model Saved ({best_acc:.2f}%)\")\n",
    "            else:\n",
    "                patience_count += 1\n",
    "                if patience_count >= PATIENCE:\n",
    "                    print(f\"â¹ï¸ Early Stopping (Epoch {epoch+1})\")\n",
    "                    break\n",
    "\n",
    "        print(f\"\\nğŸ† ìµœì¢… ì™„ë£Œ! ìµœê³  ê²€ì¦ ì •í™•ë„: {best_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9209373",
   "metadata": {},
   "source": [
    "## 2-3. íŒ€ì› ê°„ ìƒí˜¸ì‘ìš© Transformer + íŒ€ë¼ë¦¬ Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a731f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ í•™ìŠµ ì¥ì¹˜: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================================\n",
    "# [1] ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)\n",
    "# ==========================================\n",
    "DATA_FILE = 'data_v2_filtered.csv' \n",
    "MODEL_PATH = 'pubg_win_model_v4.pth'\n",
    "\n",
    "MAX_TEAMS = 10\n",
    "MAX_PLAYERS = 4\n",
    "P_COLS = ['hp', 'groggy', 'firstaid', 'smoke', 'grenade', 'helm', 'vest', 'kill', 'dX', 'dY', 'z_norm']\n",
    "T_COLS = ['num_alive', 'total_health'] \n",
    "\n",
    "PLAYER_FEATS = len(P_COLS) # 11\n",
    "TEAM_FEATS = len(T_COLS)   # 2\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° (ëª¨ë¸ì´ ë¬´ê±°ì›Œì§€ë¯€ë¡œ Hidden Dimì„ 32ë¡œ ìœ ì§€ ì¶”ì²œ)\n",
    "HIDDEN_DIM = 32      \n",
    "BATCH_SIZE = 32      \n",
    "EPOCHS = 50\n",
    "LR = 0.0005\n",
    "DROPOUT = 0.5        # ê³¼ì í•© ë°©ì§€ ìœ„í•´ ë†’ê²Œ ì„¤ì •\n",
    "PATIENCE = 10\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸ”¥ í•™ìŠµ ì¥ì¹˜: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21e2da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# [2] ë°ì´í„°ì…‹ (ìˆ˜ì •ë¨: íŒ€ ê°œìˆ˜ ì´ˆê³¼ ì‹œ ì˜ë¼ë‚´ê¸° ë¡œì§ ë³´ì™„)\n",
    "# ==========================================\n",
    "class PUBGMatchDataset(Dataset):\n",
    "    def __init__(self, df, p_scaler=None, t_scaler=None, is_train=True):\n",
    "        self.is_train = is_train\n",
    "        if is_train:\n",
    "            all_p = []\n",
    "            for i in range(1, MAX_PLAYERS+1):\n",
    "                cols = [f\"P{i}_{c}\" for c in P_COLS]\n",
    "                if all(c in df.columns for c in cols): all_p.append(df[cols].values)\n",
    "            self.p_scaler = StandardScaler()\n",
    "            if all_p: self.p_scaler.fit(np.vstack(all_p))\n",
    "            self.t_scaler = StandardScaler()\n",
    "            self.t_scaler.fit(df[T_COLS].values)\n",
    "        else:\n",
    "            self.p_scaler = p_scaler\n",
    "            self.t_scaler = t_scaler\n",
    "\n",
    "        self.groups = list(df.groupby(['match_id', 'time_sec']))\n",
    "\n",
    "    def __len__(self): return len(self.groups)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        _, group = self.groups[idx]\n",
    "        p_list = []\n",
    "        for i in range(1, MAX_PLAYERS+1):\n",
    "            cols = [f\"P{i}_{c}\" for c in P_COLS]\n",
    "            p_list.append(group[cols].fillna(0).values)\n",
    "        p_feats = np.stack(p_list, axis=1)\n",
    "        if self.p_scaler: \n",
    "            N, P, F = p_feats.shape\n",
    "            p_feats = self.p_scaler.transform(p_feats.reshape(-1, F)).reshape(N, P, F)\n",
    "\n",
    "        t_feats = group[T_COLS].fillna(0).values \n",
    "        if self.t_scaler: t_feats = self.t_scaler.transform(t_feats)\n",
    "\n",
    "        winners = group['is_winner'].values\n",
    "        target = np.argmax(winners) if np.sum(winners) > 0 else -100\n",
    "        \n",
    "        curr = p_feats.shape[0]\n",
    "        mask = torch.zeros(MAX_TEAMS, dtype=torch.bool)\n",
    "        \n",
    "        if curr < MAX_TEAMS:\n",
    "            pad = MAX_TEAMS - curr\n",
    "            p_feats = np.concatenate([p_feats, np.zeros((pad, MAX_PLAYERS, PLAYER_FEATS))], axis=0)\n",
    "            t_feats = np.concatenate([t_feats, np.zeros((pad, TEAM_FEATS))], axis=0)\n",
    "            mask[curr:] = True\n",
    "            winners = np.concatenate([winners, np.zeros(pad)])\n",
    "            \n",
    "        elif curr > MAX_TEAMS:\n",
    "            # â˜… [ìˆ˜ì •] t_featsë„ 10ê°œë¡œ ì˜ë¼ì¤˜ì•¼ í•¨!\n",
    "            p_feats = p_feats[:MAX_TEAMS]\n",
    "            t_feats = t_feats[:MAX_TEAMS]  # <-- ì´ ì¤„ì´ ì¶”ê°€ë¨\n",
    "            mask = mask[:MAX_TEAMS]\n",
    "            winners = winners[:MAX_TEAMS]\n",
    "            if target >= MAX_TEAMS: target = -100\n",
    "\n",
    "        if self.is_train:\n",
    "            perm = np.random.permutation(MAX_TEAMS)\n",
    "            p_feats = p_feats[perm]\n",
    "            t_feats = t_feats[perm] # t_featsë„ ìˆœì„œ ì„ê¸° (ì¤‘ìš”)\n",
    "            mask = mask[perm]\n",
    "            winners = winners[perm]\n",
    "            target = np.argmax(winners) if np.sum(winners) > 0 else -100\n",
    "\n",
    "        return torch.FloatTensor(p_feats), torch.FloatTensor(t_feats), mask, torch.tensor(target, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc5ad776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# [3] ëª¨ë¸ (Hierarchical Transformer)\n",
    "# ==========================================\n",
    "class HierarchicalTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. ì…ë ¥ ì„ë² ë”©\n",
    "        self.input_embed = nn.Sequential(\n",
    "            nn.Linear(PLAYER_FEATS, HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT)\n",
    "        )\n",
    "        \n",
    "        # 2. Player Transformer (íŒ€ ë‚´ë¶€ ë¶„ì„)\n",
    "        # íŒ€ì› 4ëª…ë¼ë¦¬ì˜ ìƒí˜¸ì‘ìš© ë¶„ì„\n",
    "        self.player_enc = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=HIDDEN_DIM, nhead=2, dim_feedforward=64, batch_first=True, dropout=DROPOUT),\n",
    "            num_layers=1 # ê°€ë³ê²Œ 1ì¸µ\n",
    "        )\n",
    "        \n",
    "        # 3. Match Transformer (íŒ€ ê°„ ë¶„ì„)\n",
    "        # 10ê°œ íŒ€ë¼ë¦¬ì˜ ìƒí˜¸ì‘ìš© ë¶„ì„\n",
    "        self.match_enc = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=HIDDEN_DIM, nhead=2, dim_feedforward=64, batch_first=True, dropout=DROPOUT),\n",
    "            num_layers=1 # ê°€ë³ê²Œ 1ì¸µ\n",
    "        )\n",
    "        \n",
    "        # 4. ì¶œë ¥\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(HIDDEN_DIM, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, p_x, t_x, mask):\n",
    "        # p_x: (Batch, 10, 4, 11)\n",
    "        B, T, P, F = p_x.shape\n",
    "        \n",
    "        # --- Stage 1: Player Level ---\n",
    "        # 1. ëª¨ë“  íŒ€ì˜ ëª¨ë“  í”Œë ˆì´ì–´ë¥¼ ë°°ì¹˜ë¡œ í¼ì¹¨\n",
    "        p_flat = p_x.view(B * T, P, F)   # (B*10, 4, 11)\n",
    "        \n",
    "        # 2. ì„ë² ë”©\n",
    "        p_emb = self.input_embed(p_flat) # (B*10, 4, Hidden)\n",
    "        \n",
    "        # 3. Player Transformer (íŒ€ì›ë¼ë¦¬ Attention)\n",
    "        # ê²°ê³¼: ê° í”Œë ˆì´ì–´ê°€ \"íŒ€ ìƒí™©ì„ ê³ ë ¤í•œ\" íŠ¹ì§•ì„ ê°–ê²Œ ë¨\n",
    "        p_out = self.player_enc(p_emb)   # (B*10, 4, Hidden)\n",
    "        \n",
    "        # 4. Aggregation (Sum Pooling)\n",
    "        # íŒ€ì›ë“¤ì˜ ì •ë³´ë¥¼ í•©ì³ì„œ 'íŒ€ ë²¡í„°' ìƒì„±\n",
    "        team_vec = torch.sum(p_out, dim=1) # (B*10, Hidden)\n",
    "        \n",
    "        # --- Stage 2: Match Level ---\n",
    "        # 5. ë§¤ì¹˜ ë‹¨ìœ„ë¡œ ë‹¤ì‹œ ë¬¶ìŒ\n",
    "        team_vec_match = team_vec.view(B, T, -1) # (B, 10, Hidden)\n",
    "        \n",
    "        # 6. Match Transformer (íŒ€ë¼ë¦¬ Attention)\n",
    "        # íŒ¨ë”©ëœ íŒ€ì€ ë¬´ì‹œ(mask)\n",
    "        match_out = self.match_enc(team_vec_match, src_key_padding_mask=mask) # (B, 10, Hidden)\n",
    "        \n",
    "        # 7. ì˜ˆì¸¡\n",
    "        logits = self.classifier(match_out).squeeze(-1)\n",
    "        logits = logits.masked_fill(mask, -1e9)\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7af074c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Hierarchical Transformer í•™ìŠµ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m t_loss, t_corr, t_cnt = \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m     33\u001b[39m pbar = tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEp \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qkrwl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qkrwl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qkrwl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qkrwl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mPUBGMatchDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     39\u001b[39m target = np.argmax(winners) \u001b[38;5;28;01mif\u001b[39;00m np.sum(winners) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m -\u001b[32m100\u001b[39m\n\u001b[32m     41\u001b[39m curr = p_feats.shape[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m mask = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMAX_TEAMS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m curr < MAX_TEAMS:\n\u001b[32m     45\u001b[39m     pad = MAX_TEAMS - curr\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# [4] í•™ìŠµ ì‹¤í–‰\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(DATA_FILE):\n",
    "        print(\"âŒ ë°ì´í„° íŒŒì¼ ì—†ìŒ\"); exit()\n",
    "        \n",
    "    df = pd.read_csv(DATA_FILE)\n",
    "    match_ids = df['match_id'].unique()\n",
    "    train_ids, test_ids = train_test_split(match_ids, test_size=0.2, random_state=42)\n",
    "    \n",
    "    train_df = df[df['match_id'].isin(train_ids)]\n",
    "    test_df = df[df['match_id'].isin(test_ids)]\n",
    "    \n",
    "    train_ds = PUBGMatchDataset(train_df, is_train=True)\n",
    "    test_ds = PUBGMatchDataset(test_df, p_scaler=train_ds.p_scaler, t_scaler=train_ds.t_scaler, is_train=False)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = HierarchicalTransformer().to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4) # ê·œì œ ê°•í™”\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "    \n",
    "    print(\"\\nğŸš€ Hierarchical Transformer í•™ìŠµ ì‹œì‘...\")\n",
    "    best_acc = 0; patience = 0\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        t_loss, t_corr, t_cnt = 0, 0, 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Ep {epoch+1}\", leave=False)\n",
    "        \n",
    "        for p, t, m, y in pbar:\n",
    "            p, t, m, y = p.to(device), t.to(device), m.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(p, t, m)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward(); optimizer.step()\n",
    "            \n",
    "            t_loss += loss.item()\n",
    "            pred = torch.argmax(out, 1)\n",
    "            valid = y != -100\n",
    "            t_corr += (pred[valid] == y[valid]).sum().item()\n",
    "            t_cnt += valid.sum().item()\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "            \n",
    "        train_acc = 100 * t_corr / t_cnt if t_cnt else 0\n",
    "        \n",
    "        model.eval()\n",
    "        v_corr, v_cnt = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for p, t, m, y in test_loader:\n",
    "                p, t, m, y = p.to(device), t.to(device), m.to(device), y.to(device)\n",
    "                out = model(p, t, m)\n",
    "                pred = torch.argmax(out, 1)\n",
    "                valid = y != -100\n",
    "                v_corr += (pred[valid] == y[valid]).sum().item()\n",
    "                v_cnt += valid.sum().item()\n",
    "        val_acc = 100 * v_corr / v_cnt if v_cnt else 0\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        print(f\"Ep {epoch+1:02d} | Loss: {t_loss/len(train_loader):.4f} | Tr: {train_acc:.2f}% | Val: {val_acc:.2f}%\")\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc; patience = 0\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= PATIENCE: print(\"â¹ï¸ Early Stopping\"); break\n",
    "\n",
    "    print(f\"\\nğŸ† ì™„ë£Œ! Best Val Acc: {best_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c9352b",
   "metadata": {},
   "source": [
    "# **3. ì‹¤ì œ ë°ì´í„° ë„£ê¸°**\n",
    "ëŒ€ìƒ ëª¨ë¸: 2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c7defa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (Device: cuda)\n",
      "ğŸš€ 'jinu.jsonl' ë¶„ì„ ì¤‘...\n",
      "âœ… ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ (96 ìŠ¤ëƒ…ìƒ·)\n",
      "ğŸ“Š ìŠ¹ë¥  ê³„ì‚° ì¤‘...\n",
      "âŒ ì˜¤ë¥˜ ë°œìƒ: query should be unbatched 2D or batched 3D tensor but received 4-D query tensor\n",
      "âœ… ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ (96 ìŠ¤ëƒ…ìƒ·)\n",
      "ğŸ“Š ìŠ¹ë¥  ê³„ì‚° ì¤‘...\n",
      "âŒ ì˜¤ë¥˜ ë°œìƒ: query should be unbatched 2D or batched 3D tensor but received 4-D query tensor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\qkrwl\\AppData\\Local\\Temp\\ipykernel_36304\\3048733730.py\", line 92, in <module>\n",
      "    pred = model(p_in, t_in)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\qkrwl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\qkrwl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\qkrwl\\AppData\\Local\\Temp\\ipykernel_36304\\3810168750.py\", line 38, in forward\n",
      "    p_out = self.p_enc(p_emb)\n",
      "      ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\qkrwl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\qkrwl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\qkrwl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "             ^^^^\n",
      "  File \"c:\\Users\\qkrwl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\qkrwl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\qkrwl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\qkrwl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "        ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\qkrwl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\qkrwl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\qkrwl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\qkrwl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py\", line 6224, in multi_head_attention_forward\n",
      "    is_batched = _mha_shape_check(\n",
      "                 ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\qkrwl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py\", line 6020, in _mha_shape_check\n",
      "    raise AssertionError(\n",
      "AssertionError: query should be unbatched 2D or batched 3D tensor but received 4-D query tensor\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# ==========================================\n",
    "# [ì‹¤í–‰ë¶€] ì‹¤ì œ ë°ì´í„° ì¶”ë¡  (v1 ê¸°ë°˜)\n",
    "# ==========================================\n",
    "TARGET_LOG_FILE = 'jinu.jsonl'\n",
    "MODEL_PATH = 'pubg_win_model_v2_datav3.pth'\n",
    "\n",
    "# 2-1 ëª¨ë¸ ë³€ìˆ˜ êµ¬ì„± (17ê°œ)\n",
    "P_COLS = [\n",
    "    'hp', 'groggy',\n",
    "    'medkit', 'firstaid', 'painkiller', 'drink', 'adrenaline',\n",
    "    'smoke', 'grenade', 'flash', 'molotov',\n",
    "    'helm', 'vest',\n",
    "    'kill',\n",
    "    'dX', 'dY', 'z_norm'\n",
    "]\n",
    "T_COLS = ['num_alive', 'total_health']\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # í°íŠ¸ ì„¤ì • (í•œê¸€ ê¹¨ì§ ë°©ì§€)\n",
    "    if os.name == 'nt': \n",
    "        plt.rc('font', family='Malgun Gothic')\n",
    "    plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "    if not os.path.exists(TARGET_LOG_FILE):\n",
    "        print(f\"âŒ ë¶„ì„í•  íŒŒì¼({TARGET_LOG_FILE})ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    elif not os.path.exists(MODEL_PATH):\n",
    "        print(f\"âŒ ëª¨ë¸ íŒŒì¼({MODEL_PATH})ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        # 1. ëª¨ë¸ ë¡œë“œ (weights_only=False)\n",
    "        try:\n",
    "            checkpoint = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "            \n",
    "            model = WinPredictor().to(device)\n",
    "            model.load_state_dict(checkpoint['model'])\n",
    "            p_scaler = checkpoint['p_scaler']\n",
    "            t_scaler = checkpoint['t_scaler']\n",
    "            model.eval()\n",
    "            \n",
    "            print(f\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (Device: {device})\")\n",
    "            \n",
    "            # 2. ë°ì´í„° ì¶”ì¶œ\n",
    "            print(f\"ğŸš€ '{TARGET_LOG_FILE}' ë¶„ì„ ì¤‘...\")\n",
    "            df = process_match_log(TARGET_LOG_FILE)\n",
    "            \n",
    "            if df is None or df.empty:\n",
    "                print(\"âŒ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨\")\n",
    "            else:\n",
    "                df.to_csv(\"jinu_example.csv\", index=False)\n",
    "                print(f\"âœ… ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ ({len(df)} ìŠ¤ëƒ…ìƒ·)\")\n",
    "\n",
    "                # 3. ì¶”ë¡  (Inference)\n",
    "                time_steps = df['time_sec'].unique()\n",
    "                history_probs = []\n",
    "                \n",
    "                # ì´ˆê¸° ìƒì¡´ íŒ€ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë˜í”„ ìƒ‰ìƒ/ìˆœì„œ ê³ ì •\n",
    "                initial_teams = df[df['time_sec'] == time_steps[0]]['team_id'].values\n",
    "                team_to_idx = {tid: i for i, tid in enumerate(initial_teams)}\n",
    "\n",
    "                print(\"ğŸ“Š ìŠ¹ë¥  ê³„ì‚° ì¤‘...\")\n",
    "                with torch.no_grad():\n",
    "                    for t_sec in time_steps:\n",
    "                        step_df = df[df['time_sec'] == t_sec]\n",
    "                        \n",
    "                        # ê° ì‹œê°„ëŒ€ë³„ íŒ€ì˜ ìŠ¹ë¥ ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "                        time_probs = []\n",
    "                        \n",
    "                        # ê° íŒ€ë§ˆë‹¤ ê°œë³„ë¡œ ì˜ˆì¸¡\n",
    "                        for tid in team_to_idx.keys():\n",
    "                            team_row = step_df[step_df['team_id'] == tid]\n",
    "                            \n",
    "                            if team_row.empty:\n",
    "                                time_probs.append(0.0)  # íŒ€ì´ ì—†ìœ¼ë©´ 0\n",
    "                                continue\n",
    "                            \n",
    "                            row = team_row.iloc[0]\n",
    "                            \n",
    "                            # Player Data (4ëª…, 17ê°œ íŠ¹ì§•)\n",
    "                            p_data = []\n",
    "                            for i in range(4):\n",
    "                                cols = [f\"P{i+1}_{c}\" for c in P_COLS]\n",
    "                                raw_p = row[cols].values.reshape(1, -1)\n",
    "                                p_data.append(p_scaler.transform(raw_p))\n",
    "                            p_tensor = np.stack(p_data, axis=0)  # (4, 17)\n",
    "                            \n",
    "                            # Team Data (2ê°œ íŠ¹ì§•)\n",
    "                            raw_t = row[T_COLS].values.reshape(1, -1)\n",
    "                            t_tensor = t_scaler.transform(raw_t)  # (1, 2)\n",
    "                            \n",
    "                            # Batch ì°¨ì› ì¶”ê°€: (1, 4, 17), (1, 2)\n",
    "                            p_in = torch.FloatTensor(p_tensor).unsqueeze(0).to(device)\n",
    "                            t_in = torch.FloatTensor(t_tensor).to(device)\n",
    "                            \n",
    "                            # ëª¨ë¸ ì‹¤í–‰\n",
    "                            pred = model(p_in, t_in)\n",
    "                            prob = pred.cpu().numpy()[0][0]  # Sigmoid ì¶œë ¥ê°’\n",
    "                            time_probs.append(prob)\n",
    "                        \n",
    "                        history_probs.append(time_probs)\n",
    "\n",
    "                # 4. ì‹œê°í™”\n",
    "                history_probs = np.array(history_probs).T  # (Teams, Time)\n",
    "                \n",
    "                # ë§ˆì§€ë§‰ ì‹œì  ìŠ¹ë¥  ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬ (Top 5)\n",
    "                last_probs = history_probs[:, -1]\n",
    "                top_indices = np.argsort(last_probs)[::-1][:5]\n",
    "                \n",
    "                plt.figure(figsize=(12, 6))\n",
    "                labels = [f\"Team {list(team_to_idx.keys())[i]}\" for i in top_indices]\n",
    "                \n",
    "                # Stackplot\n",
    "                plt.stackplot(time_steps, history_probs[top_indices], labels=labels, alpha=0.85)\n",
    "                \n",
    "                plt.title(f\"ì‹¤ì‹œê°„ ìŠ¹ë¥  ë³€í™” (Match: {TARGET_LOG_FILE})\", fontsize=15, fontweight='bold')\n",
    "                plt.xlabel(\"ê²Œì„ ì§„í–‰ ì‹œê°„ (ì´ˆ)\", fontsize=12)\n",
    "                plt.ylabel(\"ìš°ìŠ¹ í™•ë¥ \", fontsize=12)\n",
    "                plt.legend(loc='upper left', bbox_to_anchor=(1, 1), title=\"Top Teams\")\n",
    "                plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                plt.show()\n",
    "                print(\"âœ¨ ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
